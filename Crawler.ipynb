{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing crawler.py\n"
     ]
    }
   ],
   "source": [
    "#%%writefile 'crawler.py'\n",
    "# coding: utf-8\n",
    "\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from csv import writer\n",
    "import numpy as np\n",
    "\n",
    "def colist(url = 'http://kind.krx.co.kr/corpgeneral/corpList.do?method=download&searchType=13', write = False):\n",
    "    request = urllib.request.Request(url)\n",
    "    request.get_method = lambda: 'GET'\n",
    "    response_body = urllib.request.urlopen(request).read().decode('euc-kr')\n",
    "    soup=BeautifulSoup(response_body,\"html\")\n",
    "    temp = soup.find_all('tr')\n",
    "    colist = {}\n",
    "    for i in range(1,len(temp)):\n",
    "        coname = temp[i].find_all('td')[0].get_text()\n",
    "        code = temp[i].find_all('td')[1].get_text()\n",
    "        colist[coname] = code\n",
    "        \n",
    "    if write == True:\n",
    "        w = writer(open(\"colist.csv\",\"w\"))\n",
    "        for key, val in colist.items():\n",
    "            w.writerow([key,val])\n",
    "    return colist\n",
    "\n",
    "\n",
    "\n",
    "# X변수\n",
    "def Xcrawler(co_list,co_name='삼성전기', pages = 5):\n",
    "    \"\"\"\n",
    "    Xcrawler is crawling price data from naver finance page\n",
    "    colist : dict of stock\n",
    "    co_name : compnay you want\n",
    "    \"\"\"\n",
    "    code = co_list[co_name]\n",
    "    date = []\n",
    "    endp = []\n",
    "    maxp = []\n",
    "    minp = []\n",
    "    mass = []\n",
    "    page = 1\n",
    "    while page <= pages:\n",
    "        print((page-1)/pages*100)\n",
    "        url=f'https://finance.naver.com/item/sise_day.nhn?code={code}&page={page}'\n",
    "        request = urllib.request.Request(url)\n",
    "        response_body = urllib.request.urlopen(request).read().decode('euc-kr')\n",
    "        soup=BeautifulSoup(response_body,\"html\")\n",
    "        temp = soup.find_all('span', attrs={'class': 'tah p11'})\n",
    "        date_temp = soup.find_all('span', attrs={'class': 'tah p10 gray03'})\n",
    "        i = 0 \n",
    "        for i in range(1,len(date_temp)):\n",
    "            date.append(date_temp[i].get_text())\n",
    "            endp.append(temp[5*i+2].get_text())\n",
    "            maxp.append(temp[5*i+3].get_text())\n",
    "            mass.append(temp[5*i+4].get_text())\n",
    "        page +=1\n",
    "    print(100.0)\n",
    "    return date, endp, maxp, mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "10.0\n",
      "20.0\n",
      "30.0\n",
      "40.0\n",
      "50.0\n",
      "60.0\n",
      "70.0\n",
      "80.0\n",
      "90.0\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "co_list =colist()\n",
    "date, endp, maxp, mass = Xcrawler(co_list, pages = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2019.07.15',\n",
       " '2019.07.12',\n",
       " '2019.07.11',\n",
       " '2019.07.10',\n",
       " '2019.07.09',\n",
       " '2019.07.08',\n",
       " '2019.07.05',\n",
       " '2019.07.04',\n",
       " '2019.07.03',\n",
       " '2019.07.01',\n",
       " '2019.06.28',\n",
       " '2019.06.27',\n",
       " '2019.06.26',\n",
       " '2019.06.25',\n",
       " '2019.06.24',\n",
       " '2019.06.21',\n",
       " '2019.06.20',\n",
       " '2019.06.19',\n",
       " '2019.06.17',\n",
       " '2019.06.14',\n",
       " '2019.06.13',\n",
       " '2019.06.12',\n",
       " '2019.06.11',\n",
       " '2019.06.10',\n",
       " '2019.06.07',\n",
       " '2019.06.05',\n",
       " '2019.06.04',\n",
       " '2019.05.31',\n",
       " '2019.05.30',\n",
       " '2019.05.29',\n",
       " '2019.05.28',\n",
       " '2019.05.27',\n",
       " '2019.05.24',\n",
       " '2019.05.23',\n",
       " '2019.05.22',\n",
       " '2019.05.21',\n",
       " '2019.05.17',\n",
       " '2019.05.16',\n",
       " '2019.05.15',\n",
       " '2019.05.14',\n",
       " '2019.05.13',\n",
       " '2019.05.10',\n",
       " '2019.05.09',\n",
       " '2019.05.08',\n",
       " '2019.05.07',\n",
       " '2019.05.02',\n",
       " '2019.04.30',\n",
       " '2019.04.29',\n",
       " '2019.04.26',\n",
       " '2019.04.25',\n",
       " '2019.04.24',\n",
       " '2019.04.23',\n",
       " '2019.04.22',\n",
       " '2019.04.19',\n",
       " '2019.04.17',\n",
       " '2019.04.16',\n",
       " '2019.04.15',\n",
       " '2019.04.12',\n",
       " '2019.04.11',\n",
       " '2019.04.10',\n",
       " '2019.04.09',\n",
       " '2019.04.08',\n",
       " '2019.04.05',\n",
       " '2019.04.03',\n",
       " '2019.04.02',\n",
       " '2019.04.01',\n",
       " '2019.03.29',\n",
       " '2019.03.28',\n",
       " '2019.03.27',\n",
       " '2019.03.26',\n",
       " '2019.03.25',\n",
       " '2019.03.22',\n",
       " '2019.03.20',\n",
       " '2019.03.19',\n",
       " '2019.03.18',\n",
       " '2019.03.15',\n",
       " '2019.03.14',\n",
       " '2019.03.13',\n",
       " '2019.03.12',\n",
       " '2019.03.11',\n",
       " '2019.03.08',\n",
       " '2019.03.06',\n",
       " '2019.03.05',\n",
       " '2019.03.04',\n",
       " '2019.02.28',\n",
       " '2019.02.27',\n",
       " '2019.02.26',\n",
       " '2019.02.25',\n",
       " '2019.02.22',\n",
       " '2019.02.21']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
